{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gensim is a Python library for topic modelling, document indexing and similarity retrieval with large corpora. \n",
    "Target audience is the natural language processing (NLP) and information retrieval (IR) community."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://radimrehurek.com/gensim/auto_examples/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models, similarities\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 向量空间"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dictionary和corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)], [(2, 1), (8, 1), (9, 2), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1)], [(5, 1), (13, 1), (15, 1), (16, 1), (17, 1), (18, 1)], [(4, 1), (9, 1), (13, 2), (16, 1), (19, 1), (20, 1), (21, 1)], [(9, 1), (11, 1), (14, 1), (15, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1)], [(9, 1), (18, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1)], [(9, 1), (18, 1), (30, 1), (32, 1), (33, 1), (34, 1), (35, 1)], [(9, 1), (19, 1), (30, 1), (32, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 1), (41, 1)], [(8, 1), (12, 1), (32, 1), (37, 1)]]\n"
     ]
    }
   ],
   "source": [
    "def GenDictandCorpus():\n",
    "    documents = [\"Human machine interface for lab abc computer applications\",\n",
    "                 \"A survey of user opinion of computer system response time\",\n",
    "                 \"The EPS user interface management system\",\n",
    "                 \"System and human system engineering testing of EPS\",\n",
    "                 \"Relation of user perceived response time to error measurement\",\n",
    "                 \"The generation of random binary unordered trees\",\n",
    "                 \"The intersection graph of paths in trees\",\n",
    "                 \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "                 \"Graph minors A survey\"]\n",
    "\n",
    "    texts = [[word for word in document.lower().split()] for document in documents]\n",
    "\n",
    "    # 词典\n",
    "    dictionary = corpora.Dictionary(texts)\n",
    "    # 词库，以(词，词频)方式存贮   corpus将文本存贮成(词在词典中位置，词频)这种形式，每个文本为一行。\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "#     print(dictionary)\n",
    "#     print(corpus)\n",
    "    return dictionary, corpus\n",
    "dictionary, corpus = GenDictandCorpus()\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tfidf():\n",
    "    dictionary, corpus = GenDictandCorpus()\n",
    "\n",
    "    # initialize a model\n",
    "    tfidf = models.TfidfModel(corpus)\n",
    "    # print(tfidf)\n",
    "\n",
    "    # Transforming vectors\n",
    "    # 此时，tfidf被视为一个只读对象，可以用于将任何向量从旧表示（词频）转换为新表示（TfIdf实值权重）\n",
    "    doc_bow = [(0, 1), (1, 1)]\n",
    "    # 使用模型tfidf，将doc_bow(由词,词频)表示转换成(词,tfidf)表示\n",
    "    # print(tfidf[doc_bow])\n",
    "\n",
    "    # 转换整个词库\n",
    "    corpus_tfidf = tfidf[corpus]\n",
    "#     for doc in corpus_tfidf:\n",
    "#         print(doc)\n",
    "    return corpus_tfidf\n",
    "tfidf = Tfidf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LDA\n",
    "https://radimrehurek.com/gensim/models/ldamodel.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.096*\"of\" + 0.045*\"system\" + 0.044*\"graph\" + 0.044*\"trees\" + 0.036*\"user\" '\n",
      "  '+ 0.035*\"the\" + 0.033*\"response\" + 0.033*\"time\" + 0.033*\"a\" + '\n",
      "  '0.032*\"minors\"'),\n",
      " (1,\n",
      "  '0.060*\"interface\" + 0.044*\"system\" + 0.043*\"eps\" + 0.041*\"human\" + '\n",
      "  '0.038*\"the\" + 0.036*\"user\" + 0.036*\"abc\" + 0.036*\"computer\" + 0.036*\"lab\" + '\n",
      "  '0.036*\"management\"')]\n"
     ]
    }
   ],
   "source": [
    "def LDA():\n",
    "    dictionary, corpus = GenDictandCorpus()\n",
    "#     corpus_tfidf = Tfidf()\n",
    "    ldamodel = models.LdaModel(corpus, id2word=dictionary, num_topics=2)\n",
    "#     ldamodel.print_topics()\n",
    "    pprint(ldamodel.print_topics())\n",
    "    \n",
    "    return ldamodel\n",
    "lda = LDA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.14493868), (1, 0.8550613)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "other_texts = [\n",
    "    ['computer', 'time', 'graph'],\n",
    "    ['survey', 'response', 'eps'],\n",
    "    ['human', 'system', 'computer']\n",
    "    ]\n",
    "other_corpus = [dictionary.doc2bow(text) for text in other_texts]\n",
    "unseen_doc = other_corpus[0]\n",
    "vector = lda[unseen_doc]\n",
    "vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 潜在语义索引\n",
    "(Latent Semantic Indexing,以下简称LSI)，有的文章也叫Latent Semantic  Analysis（LSA）\n",
    "LSI是基于奇异值分解（SVD）的方法来得到文本的主题的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.331*\"system\" + 0.329*\"a\" + 0.329*\"survey\" + 0.241*\"user\" + 0.234*\"minors\" '\n",
      "  '+ 0.217*\"opinion\" + 0.215*\"eps\" + 0.212*\"graph\" + 0.205*\"response\" + '\n",
      "  '0.205*\"time\"'),\n",
      " (1,\n",
      "  '0.330*\"minors\" + -0.313*\"eps\" + -0.301*\"system\" + 0.288*\"graph\" + 0.274*\"a\" '\n",
      "  '+ 0.274*\"survey\" + -0.268*\"management\" + -0.262*\"interface\" + '\n",
      "  '-0.208*\"human\" + -0.189*\"testing\"')]\n",
      "[(0, 0.25053350643882555), (1, -0.36663044623753294)]\n",
      "[(0, 0.7126217528750018), (1, 0.11068666534656395)]\n",
      "[(0, 0.4918324800213249), (1, -0.5557409774957023)]\n",
      "[(0, 0.45394917960264897), (1, -0.5034737767110555)]\n",
      "[(0, 0.3371561211589134), (1, -0.02013888023727338)]\n",
      "[(0, 0.1602526256061259), (1, 0.03339166766627265)]\n",
      "[(0, 0.2620936417258959), (1, 0.22806265366020462)]\n",
      "[(0, 0.32989319215286433), (1, 0.40778939730084907)]\n",
      "[(0, 0.5563151509435186), (1, 0.5788024705523834)]\n"
     ]
    }
   ],
   "source": [
    "def LSI():\n",
    "    dictionary, corpus = GenDictandCorpus()\n",
    "    corpus_tfidf = Tfidf()\n",
    "\n",
    "    # initialize an LSI transformation\n",
    "    lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=2)\n",
    "    corpus_lsi = lsi[corpus_tfidf]\n",
    "   # print(corpus_lsi)\n",
    "    pprint(lsi.print_topics(2))\n",
    "   # 在这里实际执行了bow-> tfidf和tfidf-> lsi转换\n",
    "    for doc in corpus_lsi:\n",
    "        print(doc)\n",
    "    return lsi\n",
    "    # lsi.save('/tmp/model.lsi')\n",
    "    # lsi = models.LsiModel.load('/tmp/model.lsi')\n",
    "lsi = LSI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 随机投影"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.29435908794403076), (1, -0.08813542127609253)]\n",
      "[(0, 0.11182543635368347), (1, 0.14982929825782776)]\n",
      "[(0, 0.21567857265472412), (1, -1.078392505645752)]\n",
      "[(0, -0.6272317171096802), (1, -0.503377377986908)]\n",
      "[(0, 0.1940629482269287), (1, 0.5821888446807861)]\n",
      "[(0, -0.393359512090683), (1, 0.7254387140274048)]\n",
      "[(0, -0.6123313307762146), (1, -0.841675341129303)]\n",
      "[(0, 0.1240825355052948), (1, 0.05129450559616089)]\n",
      "[(0, -0.8537415266036987), (1, -0.1014062762260437)]\n"
     ]
    }
   ],
   "source": [
    "# 随机投影(Random Projections)，RP旨在减少矢量空间维数。\n",
    "# 这是非常有效的方法，通过投掷一点随机性来近似文档之间的TfIdf距离。\n",
    "# 推荐的目标维度数百/千，取决于您的数据集。\n",
    "def RP():\n",
    "    corpus_tfidf = Tfidf()\n",
    "    RP_model = models.RpModel(corpus_tfidf, num_topics=2)\n",
    "   # print(RP_model)\n",
    "    corpus_rp = RP_model[corpus_tfidf]\n",
    "    for doc in corpus_rp:\n",
    "        print(doc)\n",
    "    return RP_model\n",
    "rp = RP()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 相似性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform corpus to space and index it 创建索引\n",
    "index=similarities.MatrixSimilarity(lsi[corpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, 0.99545485),\n",
       " (2, 0.991444),\n",
       " (0, 0.96337396),\n",
       " (4, 0.8171174),\n",
       " (1, 0.7188792),\n",
       " (5, 0.65916455),\n",
       " (6, 0.27311423),\n",
       " (8, 0.08344132),\n",
       " (7, 0.0708783)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# search\n",
    "doc=\"human computer interaction\"  # query\n",
    "vec_bow=dictionary.doc2bow(doc.lower().split())\n",
    "vec_lsi=lsi[vec_bow]  #convert the query to LSI space\n",
    "\n",
    "sims=index[vec_lsi]\n",
    "sims=sorted(enumerate(sims),key=lambda item:-item[1])\n",
    "sims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doc2Vec\n",
    "In Gensim, we refer to the Paragraph Vector model as Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_corpus = []\n",
    "# 使用count当做每个句子的“标签”，标签和每个句子是一一对应的\n",
    "count = 0\n",
    "documents = [\n",
    "    ['computer', 'time', 'graph'],\n",
    "    ['survey', 'response', 'eps'],\n",
    "    ['human', 'system', 'computer']\n",
    "    ]\n",
    "for words in documents:\n",
    "    # 切词，返回的结果是列表类型\n",
    "#     words = segment(line)\n",
    "    # 这里documents里的每个元素是二元组，具体可以查看函数文档\n",
    "    train_corpus.append(doc2vec.TaggedDocument(words, [str(count)]))\n",
    "    count += 1\n",
    "#     if count % 10 == 0:\n",
    "#         logging.info('{} has loaded...'.format(count))\n",
    "\n",
    "model = doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=40)\n",
    "model.build_vocab(train_corpus)\n",
    "# 模型训练\n",
    "model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.00875808 -0.00194624  0.00502243 -0.00473971 -0.00615937 -0.00218707\n",
      " -0.00687608  0.00440042  0.00566705 -0.00725835 -0.0080782  -0.00899301\n",
      "  0.00635291 -0.00534122 -0.00673877  0.00519126 -0.00184545 -0.00062682\n",
      "  0.00760184  0.0056194   0.00812614  0.00067861  0.00808792  0.00721509\n",
      " -0.00352193  0.00754973  0.00659988  0.00525765  0.00172301  0.00030624\n",
      " -0.00013697  0.00058433  0.00882175  0.00545469 -0.00337192  0.00120288\n",
      "  0.00437443  0.0084803  -0.00418913 -0.00756418 -0.0009385  -0.00829048\n",
      " -0.00253594 -0.00590744  0.00769454 -0.00261715  0.00033818 -0.0016226\n",
      " -0.00664085  0.00788883]\n"
     ]
    }
   ],
   "source": [
    "vector = model.infer_vector(['only', 'you', 'can', 'prevent', 'forest', 'fires'])\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['human', 'interface', 'computer']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "common_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastText(size=4, window=3, min_count=1)\n",
    "model.build_vocab(sentences=common_texts)\n",
    "model.train(sentences=common_texts, total_examples=len(common_texts), epochs=10)  # train\n",
    "\n",
    "# # pass all the above parameters to the constructor to do everything in a single line:\n",
    "# model2 = FastText(size=4, window=3, min_count=1, sentences=common_texts, iter=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
